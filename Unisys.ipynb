{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxXH58A22tB9",
        "outputId": "3cb52203-46ca-4c68-db36-ceaa111aa853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1LCpl7tY2X4y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from scipy.ndimage import median_filter\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/Dataset_U/Glioma'\n",
        "output_directory = '/content/drive/MyDrive/Dataset_U/_preprocessed'\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "\n",
        "def weighted_median_filter(img, mask_size, weights=None):\n",
        "\n",
        "  # Input validation\n",
        "  if mask_size % 2 != 1:\n",
        "    raise ValueError(\"Mask size must be odd.\")\n",
        "\n",
        "  # Get image dimensions\n",
        "  image_height, image_width = img.shape\n",
        "\n",
        "  # Pad image for border handling (optional, consider alternative strategies)\n",
        "  pad_width = int(mask_size // 2)\n",
        "  padded_image = np.pad(img, pad_width, mode='edge')\n",
        "\n",
        "  # Create output image\n",
        "  filtered_image = np.zeros_like(img)\n",
        "\n",
        "  # Iterate through each pixel (excluding borders due to padding)\n",
        "  for y in range(pad_width, image_height + pad_width):\n",
        "    for x in range(pad_width, image_width + pad_width):\n",
        "      # Extract neighborhood around the pixel\n",
        "      window = padded_image[y - pad_width:y + pad_width + 1,\n",
        "                            x - pad_width:x + pad_width + 1]\n",
        "\n",
        "      # Flatten neighborhood and sort (consider vectorized operations for efficiency)\n",
        "      flat_window = window.flatten()\n",
        "      sorted_window = np.sort(flat_window)\n",
        "\n",
        "      # Calculate weighted median index (handle odd/even mask sizes)\n",
        "      median_index = mask_size**2 // 2\n",
        "      if mask_size % 2 == 0:  # Even mask size, average two medians\n",
        "        median_index = (median_index - 1 + median_index) // 2\n",
        "\n",
        "      # Apply weights if provided\n",
        "      if weights is not None:\n",
        "        weighted_window = sorted_window * weights.flatten()\n",
        "        median_value = weighted_window[median_index]\n",
        "      else:\n",
        "        median_value = sorted_window[median_index]\n",
        "\n",
        "      # Set filtered pixel value\n",
        "      filtered_image[y - pad_width, x - pad_width] = median_value\n",
        "\n",
        "  return filtered_image\n",
        "\n",
        "# Define mask size and weights\n",
        "mask_size = 3\n",
        "weights = None  # Use uniform weights\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    if os.path.isfile(f):\n",
        "        # Read the image\n",
        "        img = cv2.imread(f, 0)\n",
        "\n",
        "        # Resize image to 256x256\n",
        "        img_resized = resize(img, (256, 256), anti_aliasing=True)\n",
        "\n",
        "         # Apply weighted median filtering\n",
        "        filtered_image = weighted_median_filter(img_resized, mask_size, weights)\n",
        "\n",
        "  # Normalize and convert data type (if necessary)\n",
        "        filtered_image = cv2.normalize(filtered_image, filtered_image, 0, 255, cv2.NORM_MINMAX)\n",
        "        filtered_image = filtered_image.astype(np.uint8)  # Convert if not already uint8\n",
        "\n",
        "  # Save the denoised image\n",
        "        output_path = os.path.join(output_directory, '{}_denoised.jpg'.format(os.path.splitext(filename)[0]))\n",
        "        cv2.imwrite(output_path, filtered_image)\n",
        "\n",
        "        # Display the denoised image\n",
        "       # plt.imshow(filtered_image, cmap='gray')\n",
        "       # plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from scipy.ndimage import median_filter\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/Dataset_U/Meningioma'\n",
        "output_directory = '/content/drive/MyDrive/Dataset_U/_preprocessed'\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "\n",
        "def weighted_median_filter(img, mask_size, weights=None):\n",
        "\n",
        "  # Input validation\n",
        "  if mask_size % 2 != 1:\n",
        "    raise ValueError(\"Mask size must be odd.\")\n",
        "\n",
        "  # Get image dimensions\n",
        "  image_height, image_width = img.shape\n",
        "\n",
        "  # Pad image for border handling (optional, consider alternative strategies)\n",
        "  pad_width = int(mask_size // 2)\n",
        "  padded_image = np.pad(img, pad_width, mode='edge')\n",
        "\n",
        "  # Create output image\n",
        "  filtered_image = np.zeros_like(img)\n",
        "\n",
        "  # Iterate through each pixel (excluding borders due to padding)\n",
        "  for y in range(pad_width, image_height + pad_width):\n",
        "    for x in range(pad_width, image_width + pad_width):\n",
        "      # Extract neighborhood around the pixel\n",
        "      window = padded_image[y - pad_width:y + pad_width + 1,\n",
        "                            x - pad_width:x + pad_width + 1]\n",
        "\n",
        "      # Flatten neighborhood and sort (consider vectorized operations for efficiency)\n",
        "      flat_window = window.flatten()\n",
        "      sorted_window = np.sort(flat_window)\n",
        "\n",
        "      # Calculate weighted median index (handle odd/even mask sizes)\n",
        "      median_index = mask_size**2 // 2\n",
        "      if mask_size % 2 == 0:  # Even mask size, average two medians\n",
        "        median_index = (median_index - 1 + median_index) // 2\n",
        "\n",
        "      # Apply weights if provided\n",
        "      if weights is not None:\n",
        "        weighted_window = sorted_window * weights.flatten()\n",
        "        median_value = weighted_window[median_index]\n",
        "      else:\n",
        "        median_value = sorted_window[median_index]\n",
        "\n",
        "      # Set filtered pixel value\n",
        "      filtered_image[y - pad_width, x - pad_width] = median_value\n",
        "\n",
        "  return filtered_image\n",
        "\n",
        "# Define mask size and weights\n",
        "mask_size = 3\n",
        "weights = None  # Use uniform weights\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    if os.path.isfile(f):\n",
        "        # Read the image\n",
        "        img = cv2.imread(f, 0)\n",
        "\n",
        "        # Resize image to 256x256\n",
        "        img_resized = resize(img, (256, 256), anti_aliasing=True)\n",
        "\n",
        "         # Apply weighted median filtering\n",
        "        filtered_image = weighted_median_filter(img_resized, mask_size, weights)\n",
        "\n",
        "  # Normalize and convert data type (if necessary)\n",
        "        filtered_image = cv2.normalize(filtered_image, filtered_image, 0, 255, cv2.NORM_MINMAX)\n",
        "        filtered_image = filtered_image.astype(np.uint8)  # Convert if not already uint8\n",
        "\n",
        "  # Save the denoised image\n",
        "        output_path = os.path.join(output_directory, '{}_denoised.jpg'.format(os.path.splitext(filename)[0]))\n",
        "        cv2.imwrite(output_path, filtered_image)\n",
        "\n",
        "        # Display the denoised image\n",
        "       # plt.imshow(filtered_image, cmap='gray')\n",
        "       # plt.show()\n"
      ],
      "metadata": {
        "id": "frTN0mif8Ylg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from scipy.ndimage import median_filter\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/Dataset_U/No Tumor'\n",
        "output_directory = '/content/drive/MyDrive/Dataset_U/_preprocessed'\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "\n",
        "def weighted_median_filter(img, mask_size, weights=None):\n",
        "\n",
        "  # Input validation\n",
        "  if mask_size % 2 != 1:\n",
        "    raise ValueError(\"Mask size must be odd.\")\n",
        "\n",
        "  # Get image dimensions\n",
        "  image_height, image_width = img.shape\n",
        "\n",
        "  # Pad image for border handling (optional, consider alternative strategies)\n",
        "  pad_width = int(mask_size // 2)\n",
        "  padded_image = np.pad(img, pad_width, mode='edge')\n",
        "\n",
        "  # Create output image\n",
        "  filtered_image = np.zeros_like(img)\n",
        "\n",
        "  # Iterate through each pixel (excluding borders due to padding)\n",
        "  for y in range(pad_width, image_height + pad_width):\n",
        "    for x in range(pad_width, image_width + pad_width):\n",
        "      # Extract neighborhood around the pixel\n",
        "      window = padded_image[y - pad_width:y + pad_width + 1,\n",
        "                            x - pad_width:x + pad_width + 1]\n",
        "\n",
        "      # Flatten neighborhood and sort (consider vectorized operations for efficiency)\n",
        "      flat_window = window.flatten()\n",
        "      sorted_window = np.sort(flat_window)\n",
        "\n",
        "      # Calculate weighted median index (handle odd/even mask sizes)\n",
        "      median_index = mask_size**2 // 2\n",
        "      if mask_size % 2 == 0:  # Even mask size, average two medians\n",
        "        median_index = (median_index - 1 + median_index) // 2\n",
        "\n",
        "      # Apply weights if provided\n",
        "      if weights is not None:\n",
        "        weighted_window = sorted_window * weights.flatten()\n",
        "        median_value = weighted_window[median_index]\n",
        "      else:\n",
        "        median_value = sorted_window[median_index]\n",
        "\n",
        "      # Set filtered pixel value\n",
        "      filtered_image[y - pad_width, x - pad_width] = median_value\n",
        "\n",
        "  return filtered_image\n",
        "\n",
        "# Define mask size and weights\n",
        "mask_size = 3\n",
        "weights = None  # Use uniform weights\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    if os.path.isfile(f):\n",
        "        # Read the image\n",
        "        img = cv2.imread(f, 0)\n",
        "\n",
        "        # Resize image to 256x256\n",
        "        img_resized = resize(img, (256, 256), anti_aliasing=True)\n",
        "\n",
        "         # Apply weighted median filtering\n",
        "        filtered_image = weighted_median_filter(img_resized, mask_size, weights)\n",
        "\n",
        "  # Normalize and convert data type (if necessary)\n",
        "        filtered_image = cv2.normalize(filtered_image, filtered_image, 0, 255, cv2.NORM_MINMAX)\n",
        "        filtered_image = filtered_image.astype(np.uint8)  # Convert if not already uint8\n",
        "\n",
        "  # Save the denoised image\n",
        "        output_path = os.path.join(output_directory, '{}_denoised.jpg'.format(os.path.splitext(filename)[0]))\n",
        "        cv2.imwrite(output_path, filtered_image)\n",
        "\n",
        "        # Display the denoised image\n",
        "       # plt.imshow(filtered_image, cmap='gray')\n",
        "       # plt.show()\n"
      ],
      "metadata": {
        "id": "j2vuo3s79lwe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from scipy.ndimage import median_filter\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/Dataset_U/Pituitary'\n",
        "output_directory = '/content/drive/MyDrive/Dataset_U/_preprocessed'\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "\n",
        "def weighted_median_filter(img, mask_size, weights=None):\n",
        "\n",
        "  # Input validation\n",
        "  if mask_size % 2 != 1:\n",
        "    raise ValueError(\"Mask size must be odd.\")\n",
        "\n",
        "  # Get image dimensions\n",
        "  image_height, image_width = img.shape\n",
        "\n",
        "  # Pad image for border handling (optional, consider alternative strategies)\n",
        "  pad_width = int(mask_size // 2)\n",
        "  padded_image = np.pad(img, pad_width, mode='edge')\n",
        "\n",
        "  # Create output image\n",
        "  filtered_image = np.zeros_like(img)\n",
        "\n",
        "  # Iterate through each pixel (excluding borders due to padding)\n",
        "  for y in range(pad_width, image_height + pad_width):\n",
        "    for x in range(pad_width, image_width + pad_width):\n",
        "      # Extract neighborhood around the pixel\n",
        "      window = padded_image[y - pad_width:y + pad_width + 1,\n",
        "                            x - pad_width:x + pad_width + 1]\n",
        "\n",
        "      # Flatten neighborhood and sort (consider vectorized operations for efficiency)\n",
        "      flat_window = window.flatten()\n",
        "      sorted_window = np.sort(flat_window)\n",
        "\n",
        "      # Calculate weighted median index (handle odd/even mask sizes)\n",
        "      median_index = mask_size**2 // 2\n",
        "      if mask_size % 2 == 0:  # Even mask size, average two medians\n",
        "        median_index = (median_index - 1 + median_index) // 2\n",
        "\n",
        "      # Apply weights if provided\n",
        "      if weights is not None:\n",
        "        weighted_window = sorted_window * weights.flatten()\n",
        "        median_value = weighted_window[median_index]\n",
        "      else:\n",
        "        median_value = sorted_window[median_index]\n",
        "\n",
        "      # Set filtered pixel value\n",
        "      filtered_image[y - pad_width, x - pad_width] = median_value\n",
        "\n",
        "  return filtered_image\n",
        "\n",
        "# Define mask size and weights\n",
        "mask_size = 3\n",
        "weights = None  # Use uniform weights\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    if os.path.isfile(f):\n",
        "        # Read the image\n",
        "        img = cv2.imread(f, 0)\n",
        "\n",
        "        # Resize image to 256x256\n",
        "        img_resized = resize(img, (256, 256), anti_aliasing=True)\n",
        "\n",
        "         # Apply weighted median filtering\n",
        "        filtered_image = weighted_median_filter(img_resized, mask_size, weights)\n",
        "\n",
        "  # Normalize and convert data type (if necessary)\n",
        "        filtered_image = cv2.normalize(filtered_image, filtered_image, 0, 255, cv2.NORM_MINMAX)\n",
        "        filtered_image = filtered_image.astype(np.uint8)  # Convert if not already uint8\n",
        "\n",
        "  # Save the denoised image\n",
        "        output_path = os.path.join(output_directory, '{}_denoised.jpg'.format(os.path.splitext(filename)[0]))\n",
        "        cv2.imwrite(output_path, filtered_image)\n",
        "\n",
        "        # Display the denoised image\n",
        "       # plt.imshow(filtered_image, cmap='gray')\n",
        "       # plt.show()\n"
      ],
      "metadata": {
        "id": "WvQxc_4h9u3c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eKinPGcd2UV2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4a985649-2396-4081-d030-11b19a5e7cae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import matplotlib.pyplot as plt\\nfrom skimage.feature import greycomatrix, greycoprops\\nimport cv2\\nimport numpy as np\\nimport os\\n\\n# Specify the path to your image folder\\nimage_folder = \"/content/drive/MyDrive/Dataset/glioma_tumor\"\\n\\n# Define feature names as a list\\nfeature_names = [\"Energy\", \"Entropy\", \"Correlation\", \"Contrast\", \"Homogeneity\", \"ASM\"]\\n\\n# Create an empty list to store feature values for all images\\nall_features = []\\n\\n# Iterate through each image file in the folder\\nfor filename in os.listdir(image_folder):\\n    if filename.endswith(\".jpg\"):\\n        image_path = os.path.join(image_folder, filename)\\n\\n        # Read the image\\n        image = cv2.imread(image_path)\\n\\n        # Convert the image to grayscale\\n        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n\\n        # Compute GLCM\\n        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\\n\\n        # Compute GLCM properties\\n        corr = greycoprops(glcm, \\'correlation\\')[0,0]\\n        homogen = greycoprops(glcm, \\'homogeneity\\')[0,0]  # This represents IDM\\n        energy = greycoprops(glcm, \\'energy\\')[0,0]\\n        contrast = greycoprops(glcm, \\'contrast\\')[0,0]\\n\\n        # Compute Angular Second Moment (ASM)\\n        ASM = np.sum(glcm ** 2)\\n\\n        # Compute entropy\\n        probabilities = glcm / np.sum(glcm)\\n        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))  # Adding a small value to prevent log(0)\\n\\n        # Compute mean\\n        mean = np.sum(probabilities * np.arange(256))\\n\\n        # Compute variance\\n        variance = np.sum(probabilities * (np.arange(256) - mean) ** 2)\\n\\n        # Create a list to store feature values for this image\\n        image_features = [energy, entropy/10, corr, contrast/1000, homogen, ASM,1]\\n\\n        # Append the feature list for this image to the all_features list\\n        all_features.append(image_features)\\n\\n# Define the output filename (replace with your desired name)\\noutput_filename = \"glcm_features1.txt\"\\n\\n# Open the text file in write mode\\nwith open(output_filename, \"w\") as textfile:\\n\\n\\n\\n    # Write each image\\'s features as a separate line\\n    for image_features in all_features:\\n        feature_string = \"\\t\".join([str(f) for f in image_features])  # Convert each feature to string\\n        textfile.write(f\"{feature_string}\\n\")\\n\\n# Print the output file path\\nprint(f\"GLCM feature table saved to: {os.path.abspath(output_filename)}\")\\n\\nimport matplotlib.pyplot as plt\\nfrom skimage.feature import greycomatrix, greycoprops\\nimport cv2\\nimport numpy as np\\nimport os\\n\\n# Specify the path to your image folder\\nimage_folder = \"/content/drive/MyDrive/Dataset/meningioma_tumor\"\\n\\n# Define feature names as a list\\nfeature_names = [\"Energy\", \"Entropy\", \"Correlation\", \"Contrast\", \"Homogeneity\", \"ASM\"]\\n\\n# Create an empty list to store feature values for all images\\nall_features = []\\n\\n# Iterate through each image file in the folder\\nfor filename in os.listdir(image_folder):\\n    if filename.endswith(\".jpg\"):\\n        image_path = os.path.join(image_folder, filename)\\n\\n        # Read the image\\n        image = cv2.imread(image_path)\\n\\n        # Convert the image to grayscale\\n        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n\\n        # Compute GLCM\\n        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\\n\\n        # Compute GLCM properties\\n        corr = greycoprops(glcm, \\'correlation\\')[0,0]\\n        homogen = greycoprops(glcm, \\'homogeneity\\')[0,0]  # This represents IDM\\n        energy = greycoprops(glcm, \\'energy\\')[0,0]\\n        contrast = greycoprops(glcm, \\'contrast\\')[0,0]\\n\\n        # Compute Angular Second Moment (ASM)\\n        ASM = np.sum(glcm ** 2)\\n\\n        # Compute entropy\\n        probabilities = glcm / np.sum(glcm)\\n        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))  # Adding a small value to prevent log(0)\\n\\n        # Compute mean\\n        mean = np.sum(probabilities * np.arange(256))\\n\\n        # Compute variance\\n        variance = np.sum(probabilities * (np.arange(256) - mean) ** 2)\\n\\n        # Create a list to store feature values for this image\\n        image_features = [energy, entropy/10, corr, contrast/1000, homogen, ASM,2]\\n\\n        # Append the feature list for this image to the all_features list\\n        all_features.append(image_features)\\n\\n# Define the output filename (replace with your desired name)\\noutput_filename = \"glcm_features2.txt\"\\n\\n# Open the text file in write mode\\nwith open(output_filename, \"w\") as textfile:\\n\\n\\n\\n    # Write each image\\'s features as a separate line\\n    for image_features in all_features:\\n        feature_string = \"\\t\".join([str(f) for f in image_features])  # Convert each feature to string\\n        textfile.write(f\"{feature_string}\\n\")\\n\\n# Print the output file path\\nprint(f\"GLCM feature table saved to: {os.path.abspath(output_filename)}\")\\n\\nimport matplotlib.pyplot as plt\\nfrom skimage.feature import greycomatrix, greycoprops\\nimport cv2\\nimport numpy as np\\nimport os\\n\\n# Specify the path to your image folder\\nimage_folder = \"/content/drive/MyDrive/Dataset/pituitary_tumor\"\\n\\n# Define feature names as a list\\nfeature_names = [\"Energy\", \"Entropy\", \"Correlation\", \"Contrast\", \"Homogeneity\", \"ASM\"]\\n\\n# Create an empty list to store feature values for all images\\nall_features = []\\n\\n# Iterate through each image file in the folder\\nfor filename in os.listdir(image_folder):\\n    if filename.endswith(\".jpg\"):\\n        image_path = os.path.join(image_folder, filename)\\n\\n        # Read the image\\n        image = cv2.imread(image_path)\\n\\n        # Convert the image to grayscale\\n        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n\\n        # Compute GLCM\\n        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\\n\\n        # Compute GLCM properties\\n        corr = greycoprops(glcm, \\'correlation\\')[0,0]\\n        homogen = greycoprops(glcm, \\'homogeneity\\')[0,0]  # This represents IDM\\n        energy = greycoprops(glcm, \\'energy\\')[0,0]\\n        contrast = greycoprops(glcm, \\'contrast\\')[0,0]\\n\\n        # Compute Angular Second Moment (ASM)\\n        ASM = np.sum(glcm ** 2)\\n\\n        # Compute entropy\\n        probabilities = glcm / np.sum(glcm)\\n        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))  # Adding a small value to prevent log(0)\\n\\n        # Compute mean\\n        mean = np.sum(probabilities * np.arange(256))\\n\\n        # Compute variance\\n        variance = np.sum(probabilities * (np.arange(256) - mean) ** 2)\\n\\n        # Create a list to store feature values for this image\\n        image_features = [energy, entropy/10, corr, contrast/1000, homogen, ASM,3]\\n\\n        # Append the feature list for this image to the all_features list\\n        all_features.append(image_features)\\n\\n# Define the output filename (replace with your desired name)\\noutput_filename = \"glcm_features3.txt\"\\n\\n# Open the text file in write mode\\nwith open(output_filename, \"w\") as textfile:\\n\\n\\n\\n    # Write each image\\'s features as a separate line\\n    for image_features in all_features:\\n        feature_string = \"\\t\".join([str(f) for f in image_features])  # Convert each feature to string\\n        textfile.write(f\"{feature_string}\\n\")\\n\\n# Print the output file path\\nprint(f\"GLCM feature table saved to: {os.path.abspath(output_filename)}\")\\n\\nimport matplotlib.pyplot as plt\\nfrom skimage.feature import greycomatrix, greycoprops\\nimport cv2\\nimport numpy as np\\nimport os\\n\\n# Specify the path to your image folder\\nimage_folder = \"/content/drive/MyDrive/Dataset/no_tumor\"\\n\\n# Define feature names as a list\\nfeature_names = [\"Energy\", \"Entropy\", \"Correlation\", \"Contrast\", \"Homogeneity\", \"ASM\"]\\n\\n# Create an empty list to store feature values for all images\\nall_features = []\\n\\n# Iterate through each image file in the folder\\nfor filename in os.listdir(image_folder):\\n    if filename.endswith(\".jpg\"):\\n        image_path = os.path.join(image_folder, filename)\\n\\n        # Read the image\\n        image = cv2.imread(image_path)\\n\\n        # Convert the image to grayscale\\n        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n\\n        # Compute GLCM\\n        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\\n\\n        # Compute GLCM properties\\n        corr = greycoprops(glcm, \\'correlation\\')[0,0]\\n        homogen = greycoprops(glcm, \\'homogeneity\\')[0,0]  # This represents IDM\\n        energy = greycoprops(glcm, \\'energy\\')[0,0]\\n        contrast = greycoprops(glcm, \\'contrast\\')[0,0]\\n\\n        # Compute Angular Second Moment (ASM)\\n        ASM = np.sum(glcm ** 2)\\n\\n        # Compute entropy\\n        probabilities = glcm / np.sum(glcm)\\n        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))  # Adding a small value to prevent log(0)\\n\\n        # Compute mean\\n        mean = np.sum(probabilities * np.arange(256))\\n\\n        # Compute variance\\n        variance = np.sum(probabilities * (np.arange(256) - mean) ** 2)\\n\\n        # Create a list to store feature values for this image\\n        image_features = [energy, entropy/10, corr, contrast/1000, homogen, ASM,4]\\n\\n        # Append the feature list for this image to the all_features list\\n        all_features.append(image_features)\\n\\n# Define the output filename (replace with your desired name)\\noutput_filename = \"glcm_features4.txt\"\\n\\n# Open the text file in write mode\\nwith open(output_filename, \"w\") as textfile:\\n\\n\\n\\n    # Write each image\\'s features as a separate line\\n    for image_features in all_features:\\n        feature_string = \"\\t\".join([str(f) for f in image_features])  # Convert each feature to string\\n        textfile.write(f\"{feature_string}\\n\")\\n\\n# Print the output file path\\nprint(f\"GLCM feature table saved to: {os.path.abspath(output_filename)}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "'''import matplotlib.pyplot as plt\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Specify the path to your image folder\n",
        "image_folder = \"/content/drive/MyDrive/Dataset/glioma_tumor\"\n",
        "\n",
        "# Define feature names as a list\n",
        "feature_names = [\"Energy\", \"Entropy\", \"Correlation\", \"Contrast\", \"Homogeneity\", \"ASM\"]\n",
        "\n",
        "# Create an empty list to store feature values for all images\n",
        "all_features = []\n",
        "\n",
        "# Iterate through each image file in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Convert the image to grayscale\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Compute GLCM\n",
        "        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\n",
        "\n",
        "        # Compute GLCM properties\n",
        "        corr = greycoprops(glcm, 'correlation')[0,0]\n",
        "        homogen = greycoprops(glcm, 'homogeneity')[0,0]  # This represents IDM\n",
        "        energy = greycoprops(glcm, 'energy')[0,0]\n",
        "        contrast = greycoprops(glcm, 'contrast')[0,0]\n",
        "\n",
        "        # Compute Angular Second Moment (ASM)\n",
        "        ASM = np.sum(glcm ** 2)\n",
        "\n",
        "        # Compute entropy\n",
        "        probabilities = glcm / np.sum(glcm)\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))  # Adding a small value to prevent log(0)\n",
        "\n",
        "        # Compute mean\n",
        "        mean = np.sum(probabilities * np.arange(256))\n",
        "\n",
        "        # Compute variance\n",
        "        variance = np.sum(probabilities * (np.arange(256) - mean) ** 2)\n",
        "\n",
        "        # Create a list to store feature values for this image\n",
        "        image_features = [energy, entropy/10, corr, contrast/1000, homogen, ASM,1]\n",
        "\n",
        "        # Append the feature list for this image to the all_features list\n",
        "        all_features.append(image_features)\n",
        "\n",
        "# Define the output filename (replace with your desired name)\n",
        "output_filename = \"glcm_features1.txt\"\n",
        "\n",
        "# Open the text file in write mode\n",
        "with open(output_filename, \"w\") as textfile:\n",
        "\n",
        "\n",
        "\n",
        "    # Write each image's features as a separate line\n",
        "    for image_features in all_features:\n",
        "        feature_string = \"\\t\".join([str(f) for f in image_features])  # Convert each feature to string\n",
        "        textfile.write(f\"{feature_string}\\n\")\n",
        "\n",
        "# Print the output file path\n",
        "print(f\"GLCM feature table saved to: {os.path.abspath(output_filename)}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Specify the path to your image folder\n",
        "image_folder = \"/content/drive/MyDrive/Dataset/meningioma_tumor\"\n",
        "\n",
        "# Define feature names as a list\n",
        "feature_names = [\"Energy\", \"Entropy\", \"Correlation\", \"Contrast\", \"Homogeneity\", \"ASM\"]\n",
        "\n",
        "# Create an empty list to store feature values for all images\n",
        "all_features = []\n",
        "\n",
        "# Iterate through each image file in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Convert the image to grayscale\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Compute GLCM\n",
        "        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\n",
        "\n",
        "        # Compute GLCM properties\n",
        "        corr = greycoprops(glcm, 'correlation')[0,0]\n",
        "        homogen = greycoprops(glcm, 'homogeneity')[0,0]  # This represents IDM\n",
        "        energy = greycoprops(glcm, 'energy')[0,0]\n",
        "        contrast = greycoprops(glcm, 'contrast')[0,0]\n",
        "\n",
        "        # Compute Angular Second Moment (ASM)\n",
        "        ASM = np.sum(glcm ** 2)\n",
        "\n",
        "        # Compute entropy\n",
        "        probabilities = glcm / np.sum(glcm)\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))  # Adding a small value to prevent log(0)\n",
        "\n",
        "        # Compute mean\n",
        "        mean = np.sum(probabilities * np.arange(256))\n",
        "\n",
        "        # Compute variance\n",
        "        variance = np.sum(probabilities * (np.arange(256) - mean) ** 2)\n",
        "\n",
        "        # Create a list to store feature values for this image\n",
        "        image_features = [energy, entropy/10, corr, contrast/1000, homogen, ASM,2]\n",
        "\n",
        "        # Append the feature list for this image to the all_features list\n",
        "        all_features.append(image_features)\n",
        "\n",
        "# Define the output filename (replace with your desired name)\n",
        "output_filename = \"glcm_features2.txt\"\n",
        "\n",
        "# Open the text file in write mode\n",
        "with open(output_filename, \"w\") as textfile:\n",
        "\n",
        "\n",
        "\n",
        "    # Write each image's features as a separate line\n",
        "    for image_features in all_features:\n",
        "        feature_string = \"\\t\".join([str(f) for f in image_features])  # Convert each feature to string\n",
        "        textfile.write(f\"{feature_string}\\n\")\n",
        "\n",
        "# Print the output file path\n",
        "print(f\"GLCM feature table saved to: {os.path.abspath(output_filename)}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Specify the path to your image folder\n",
        "image_folder = \"/content/drive/MyDrive/Dataset/pituitary_tumor\"\n",
        "\n",
        "# Define feature names as a list\n",
        "feature_names = [\"Energy\", \"Entropy\", \"Correlation\", \"Contrast\", \"Homogeneity\", \"ASM\"]\n",
        "\n",
        "# Create an empty list to store feature values for all images\n",
        "all_features = []\n",
        "\n",
        "# Iterate through each image file in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Convert the image to grayscale\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Compute GLCM\n",
        "        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\n",
        "\n",
        "        # Compute GLCM properties\n",
        "        corr = greycoprops(glcm, 'correlation')[0,0]\n",
        "        homogen = greycoprops(glcm, 'homogeneity')[0,0]  # This represents IDM\n",
        "        energy = greycoprops(glcm, 'energy')[0,0]\n",
        "        contrast = greycoprops(glcm, 'contrast')[0,0]\n",
        "\n",
        "        # Compute Angular Second Moment (ASM)\n",
        "        ASM = np.sum(glcm ** 2)\n",
        "\n",
        "        # Compute entropy\n",
        "        probabilities = glcm / np.sum(glcm)\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))  # Adding a small value to prevent log(0)\n",
        "\n",
        "        # Compute mean\n",
        "        mean = np.sum(probabilities * np.arange(256))\n",
        "\n",
        "        # Compute variance\n",
        "        variance = np.sum(probabilities * (np.arange(256) - mean) ** 2)\n",
        "\n",
        "        # Create a list to store feature values for this image\n",
        "        image_features = [energy, entropy/10, corr, contrast/1000, homogen, ASM,3]\n",
        "\n",
        "        # Append the feature list for this image to the all_features list\n",
        "        all_features.append(image_features)\n",
        "\n",
        "# Define the output filename (replace with your desired name)\n",
        "output_filename = \"glcm_features3.txt\"\n",
        "\n",
        "# Open the text file in write mode\n",
        "with open(output_filename, \"w\") as textfile:\n",
        "\n",
        "\n",
        "\n",
        "    # Write each image's features as a separate line\n",
        "    for image_features in all_features:\n",
        "        feature_string = \"\\t\".join([str(f) for f in image_features])  # Convert each feature to string\n",
        "        textfile.write(f\"{feature_string}\\n\")\n",
        "\n",
        "# Print the output file path\n",
        "print(f\"GLCM feature table saved to: {os.path.abspath(output_filename)}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Specify the path to your image folder\n",
        "image_folder = \"/content/drive/MyDrive/Dataset/no_tumor\"\n",
        "\n",
        "# Define feature names as a list\n",
        "feature_names = [\"Energy\", \"Entropy\", \"Correlation\", \"Contrast\", \"Homogeneity\", \"ASM\"]\n",
        "\n",
        "# Create an empty list to store feature values for all images\n",
        "all_features = []\n",
        "\n",
        "# Iterate through each image file in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Convert the image to grayscale\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Compute GLCM\n",
        "        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\n",
        "\n",
        "        # Compute GLCM properties\n",
        "        corr = greycoprops(glcm, 'correlation')[0,0]\n",
        "        homogen = greycoprops(glcm, 'homogeneity')[0,0]  # This represents IDM\n",
        "        energy = greycoprops(glcm, 'energy')[0,0]\n",
        "        contrast = greycoprops(glcm, 'contrast')[0,0]\n",
        "\n",
        "        # Compute Angular Second Moment (ASM)\n",
        "        ASM = np.sum(glcm ** 2)\n",
        "\n",
        "        # Compute entropy\n",
        "        probabilities = glcm / np.sum(glcm)\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))  # Adding a small value to prevent log(0)\n",
        "\n",
        "        # Compute mean\n",
        "        mean = np.sum(probabilities * np.arange(256))\n",
        "\n",
        "        # Compute variance\n",
        "        variance = np.sum(probabilities * (np.arange(256) - mean) ** 2)\n",
        "\n",
        "        # Create a list to store feature values for this image\n",
        "        image_features = [energy, entropy/10, corr, contrast/1000, homogen, ASM,4]\n",
        "\n",
        "        # Append the feature list for this image to the all_features list\n",
        "        all_features.append(image_features)\n",
        "\n",
        "# Define the output filename (replace with your desired name)\n",
        "output_filename = \"glcm_features4.txt\"\n",
        "\n",
        "# Open the text file in write mode\n",
        "with open(output_filename, \"w\") as textfile:\n",
        "\n",
        "\n",
        "\n",
        "    # Write each image's features as a separate line\n",
        "    for image_features in all_features:\n",
        "        feature_string = \"\\t\".join([str(f) for f in image_features])  # Convert each feature to string\n",
        "        textfile.write(f\"{feature_string}\\n\")\n",
        "\n",
        "# Print the output file path\n",
        "print(f\"GLCM feature table saved to: {os.path.abspath(output_filename)}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3Sheqg42eXj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Specify the path to your image folders\n",
        "no_tumor_folder = \"/content/drive/MyDrive/Dataset_U/NoTumor_preprocessed\"\n",
        "tumor_folder = \"/content/drive/MyDrive/Dataset_U/_preprocessed\"\n",
        "\n",
        "# Define feature names as a list\n",
        "feature_names = [\"Energy\", \"Entropy\", \"Correlation\", \"Contrast\", \"Homogeneity\", \"ASM\"]\n",
        "\n",
        "# Create an empty list to store feature values for all images\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "# Load features and labels for \"No Tumor\" images\n",
        "for filename in os.listdir(no_tumor_folder):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(no_tumor_folder, filename)\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "        # Convert the image to grayscale\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        # Compute GLCM\n",
        "        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\n",
        "        # Compute GLCM properties\n",
        "        corr = greycoprops(glcm, 'correlation')[0,0]\n",
        "        homogen = greycoprops(glcm, 'homogeneity')[0,0]\n",
        "        energy = greycoprops(glcm, 'energy')[0,0]\n",
        "        contrast = greycoprops(glcm, 'contrast')[0,0]\n",
        "        ASM = np.sum(glcm ** 2)\n",
        "        probabilities = glcm / np.sum(glcm)\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
        "        # Append features and label\n",
        "        all_features.append([energy, entropy/10, corr, contrast/1000, homogen, ASM])\n",
        "        all_labels.append(0)  # Label 0 for \"No Tumor\"\n",
        "\n",
        "# Load features and labels for \"Tumor\" images\n",
        "for filename in os.listdir(tumor_folder):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(tumor_folder, filename)\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "        # Convert the image to grayscale\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        # Compute GLCM\n",
        "        glcm = greycomatrix(gray_image, [5], [np.pi/2], levels=256, symmetric=True, normed=True)\n",
        "        # Compute GLCM properties\n",
        "        corr = greycoprops(glcm, 'correlation')[0,0]\n",
        "        homogen = greycoprops(glcm, 'homogeneity')[0,0]\n",
        "        energy = greycoprops(glcm, 'energy')[0,0]\n",
        "        contrast = greycoprops(glcm, 'contrast')[0,0]\n",
        "        ASM = np.sum(glcm ** 2)\n",
        "        probabilities = glcm / np.sum(glcm)\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
        "        # Append features and label\n",
        "        all_features.append([energy, entropy/10, corr, contrast/1000, homogen, ASM])\n",
        "        all_labels.append(1)  # Label 1 for \"Tumor\"\n",
        "'''\n",
        "# Convert lists to arrays\n",
        "X = np.array(all_features)\n",
        "y = np.array(all_labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Support Vector Machine (SVM) classifier\n",
        "classifier = SVC(kernel='linear')\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUvCj6fP2oYG"
      },
      "outputs": [],
      "source": [
        "pip install scikit-fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rATDDqf829RK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def partial_dMF(x, mf_definition, partial_parameter):\n",
        "    \"\"\"Calculates the partial derivative of a membership function at a point x.\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ------\n",
        "\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "\n",
        "    \"\"\"\n",
        "    mf_name = mf_definition[0]\n",
        "\n",
        "    if mf_name == 'gaussmf':\n",
        "\n",
        "        sigma = mf_definition[1]['sigma']\n",
        "        mean = mf_definition[1]['mean']\n",
        "\n",
        "        if partial_parameter == 'sigma':\n",
        "            result = (2./sigma**3) * np.exp(-(((x-mean)**2)/(sigma)**2))*(x-mean)**2\n",
        "        elif partial_parameter == 'mean':\n",
        "            result = (2./sigma**2) * np.exp(-(((x-mean)**2)/(sigma)**2))*(x-mean)\n",
        "\n",
        "    elif mf_name == 'gbellmf':\n",
        "\n",
        "        a = mf_definition[1]['a']\n",
        "        b = mf_definition[1]['b']\n",
        "        c = mf_definition[1]['c']\n",
        "\n",
        "        if partial_parameter == 'a':\n",
        "            result = (2. * b * np.power((c-x),2) * np.power(np.absolute((c-x)/a), ((2 * b) - 2))) / \\\n",
        "                (np.power(a, 3) * np.power((np.power(np.absolute((c-x)/a),(2*b)) + 1), 2))\n",
        "        elif partial_parameter == 'b':\n",
        "            result = -1 * (2 * np.power(np.absolute((c-x)/a), (2 * b)) * np.log(np.absolute((c-x)/a))) / \\\n",
        "                (np.power((np.power(np.absolute((c-x)/a), (2 * b)) + 1), 2))\n",
        "        elif partial_parameter == 'c':\n",
        "            result = (2. * b * (c-x) * np.power(np.absolute((c-x)/a), ((2 * b) - 2))) / \\\n",
        "                (np.power(a, 2) * np.power((np.power(np.absolute((c-x)/a),(2*b)) + 1), 2))\n",
        "\n",
        "    elif mf_name == 'sigmf':\n",
        "\n",
        "        b = mf_definition[1]['b']\n",
        "        c = mf_definition[1]['c']\n",
        "\n",
        "        if partial_parameter == 'b':\n",
        "            result = -1 * (c * np.exp(c * (b + x))) / \\\n",
        "                np.power((np.exp(b*c) + np.exp(c*x)), 2)\n",
        "        elif partial_parameter == 'c':\n",
        "            result = ((x - b) * np.exp(c * (x - b))) / \\\n",
        "                np.power((np.exp(c * (x - c))) + 1, 2)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s1UDQ8xO2-LC"
      },
      "outputs": [],
      "source": [
        "\n",
        "from skfuzzy import gaussmf, gbellmf, sigmf\n",
        "\n",
        "class MemFuncs:\n",
        "    'Common base class for all employees'\n",
        "    funcDict = {'gaussmf': gaussmf, 'gbellmf': gbellmf, 'sigmf': sigmf}\n",
        "\n",
        "\n",
        "    def __init__(self, MFList):\n",
        "        self.MFList = MFList\n",
        "\n",
        "    def evaluateMF(self, rowInput):\n",
        "        if len(rowInput) != len(self.MFList):\n",
        "            print(\"Number of variables does not match number of rule sets\")\n",
        "\n",
        "        return [[self.funcDict[self.MFList[i][k][0]](rowInput[i],**self.MFList[i][k][1]) for k in range(len(self.MFList[i]))] for i in range(len(rowInput))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "caDW1fbH3CaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db812802-d785-4c16-aceb-160f26c6e192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am main!\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Apr 03 07:30:34 2014\n",
        "\n",
        "@author: tim.meggs\n",
        "\"\"\"\n",
        "import itertools\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "class ANFIS:\n",
        "    \"\"\"Class to implement an Adaptive Network Fuzzy Inference System: ANFIS\"\n",
        "\n",
        "    Attributes:\n",
        "        X\n",
        "        Y\n",
        "        XLen\n",
        "        memClass\n",
        "        memFuncs\n",
        "        memFuncsByVariable\n",
        "        rules\n",
        "        consequents\n",
        "        errors\n",
        "        memFuncsHomo\n",
        "        trainingType\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, Y, memFunction):\n",
        "        self.X = np.array(copy.copy(X))\n",
        "        self.Y = np.array(copy.copy(Y))\n",
        "        self.XLen = len(self.X)\n",
        "        self.memClass = copy.deepcopy(memFunction)\n",
        "        self.memFuncs = self.memClass.MFList\n",
        "        self.memFuncsByVariable = [[x for x in range(len(self.memFuncs[z]))] for z in range(len(self.memFuncs))]\n",
        "        self.rules = np.array(list(itertools.product(*self.memFuncsByVariable)))\n",
        "        self.consequents = np.empty(self.Y.ndim * len(self.rules) * (self.X.shape[1] + 1))\n",
        "        self.consequents.fill(0)\n",
        "        self.errors = np.empty(0)\n",
        "        self.memFuncsHomo = all(len(i)==len(self.memFuncsByVariable[0]) for i in self.memFuncsByVariable)\n",
        "        self.trainingType = 'Not trained yet'\n",
        "\n",
        "    def LSE(self, A, B, initialGamma = 1000.):\n",
        "        coeffMat = A\n",
        "        rhsMat = B\n",
        "        S = np.eye(coeffMat.shape[1])*initialGamma\n",
        "        x = np.zeros((coeffMat.shape[1],1)) # need to correct for multi-dim B\n",
        "        for i in range(len(coeffMat[:,0])):\n",
        "            a = coeffMat[i,:]\n",
        "            b = np.array(rhsMat[i])\n",
        "            S = S - (np.array(np.dot(np.dot(np.dot(S,np.matrix(a).transpose()),np.matrix(a)),S)))/(1+(np.dot(np.dot(S,a),a)))\n",
        "\n",
        "            x = x + (np.dot(S,np.dot(np.matrix(a).transpose(),(np.matrix(b)-np.dot(np.matrix(a),x)))))\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "    def trainHybridJangOffLine(self, epochs=5, tolerance=1e-5, initialGamma=1000, k=0.01):\n",
        "\n",
        "        self.trainingType = 'trainHybridJangOffLine'\n",
        "        convergence = False\n",
        "        epoch = 1\n",
        "\n",
        "        while (epoch < epochs) and (convergence is not True):\n",
        "\n",
        "            #layer four: forward pass\n",
        "            [layerFour, wSum, w] = forwardHalfPass(self, self.X)\n",
        "\n",
        "            #layer five: least squares estimate\n",
        "            layerFive = np.array(self.LSE(layerFour,self.Y,initialGamma))\n",
        "            self.consequents = layerFive\n",
        "            layerFive = np.dot(layerFour,layerFive)\n",
        "\n",
        "            #error\n",
        "            error = np.sum((self.Y-layerFive.T)**2)\n",
        "            print('current error: '+ str(error/23))\n",
        "            average_error = np.average(np.absolute(self.Y-layerFive.T))\n",
        "            self.errors = np.append(self.errors,error)\n",
        "\n",
        "            if len(self.errors) != 0:\n",
        "                if self.errors[len(self.errors)-1] < tolerance:\n",
        "                    convergence = True\n",
        "\n",
        "            # back propagation\n",
        "            if convergence is not True:\n",
        "                cols = range(len(self.X[0,:]))\n",
        "                dE_dAlpha = list(backprop(self, colX, cols, wSum, w, layerFive) for colX in range(self.X.shape[1]))\n",
        "\n",
        "\n",
        "            if len(self.errors) >= 4:\n",
        "                if (self.errors[-4] > self.errors[-3] > self.errors[-2] > self.errors[-1]):\n",
        "                    k = k * 1.1\n",
        "\n",
        "            if len(self.errors) >= 5:\n",
        "                if (self.errors[-1] < self.errors[-2]) and (self.errors[-3] < self.errors[-2]) and (self.errors[-3] < self.errors[-4]) and (self.errors[-5] > self.errors[-4]):\n",
        "                    k = k * 0.9\n",
        "\n",
        "            ## handling of variables with a different number of MFs\n",
        "            t = []\n",
        "            for x in range(len(dE_dAlpha)):\n",
        "                for y in range(len(dE_dAlpha[x])):\n",
        "                    for z in range(len(dE_dAlpha[x][y])):\n",
        "                        t.append(dE_dAlpha[x][y][z])\n",
        "\n",
        "            eta = k / np.abs(np.sum(t))\n",
        "\n",
        "            if(np.isinf(eta)):\n",
        "                eta = k\n",
        "\n",
        "            ## handling of variables with a different number of MFs\n",
        "            dAlpha = copy.deepcopy(dE_dAlpha)\n",
        "            if not(self.memFuncsHomo):\n",
        "                for x in range(len(dE_dAlpha)):\n",
        "                    for y in range(len(dE_dAlpha[x])):\n",
        "                        for z in range(len(dE_dAlpha[x][y])):\n",
        "                            dAlpha[x][y][z] = -eta * dE_dAlpha[x][y][z]\n",
        "            else:\n",
        "                dAlpha = -eta * np.array(dE_dAlpha)\n",
        "\n",
        "\n",
        "            for varsWithMemFuncs in range(len(self.memFuncs)):\n",
        "                for MFs in range(len(self.memFuncsByVariable[varsWithMemFuncs])):\n",
        "                    paramList = sorted(self.memFuncs[varsWithMemFuncs][MFs][1])\n",
        "                    for param in range(len(paramList)):\n",
        "                        self.memFuncs[varsWithMemFuncs][MFs][1][paramList[param]] = self.memFuncs[varsWithMemFuncs][MFs][1][paramList[param]] + dAlpha[varsWithMemFuncs][MFs][param]\n",
        "            epoch = epoch + 1\n",
        "\n",
        "\n",
        "        self.fittedValues = predict(self,self.X)\n",
        "        if (self.fittedValues<0).any:\n",
        "          self.fittedValues[self.fittedValues < 0] = 1\n",
        "        self.residuals = self.Y - self.fittedValues[:,0]\n",
        "\n",
        "        return self.fittedValues\n",
        "\n",
        "\n",
        "    def plotErrors(self):\n",
        "        if self.trainingType == 'Not trained yet':\n",
        "            print(self.trainingType)\n",
        "        else:\n",
        "            import matplotlib.pyplot as plt\n",
        "            plt.plot(range(len(self.errors)),self.errors,'ro', label='errors')\n",
        "            plt.ylabel('error')\n",
        "            plt.xlabel('epoch')\n",
        "            plt.show()\n",
        "\n",
        "    def plotMF(self, x, inputVar):\n",
        "        import matplotlib.pyplot as plt\n",
        "        from skfuzzy import gaussmf, gbellmf, sigmf\n",
        "\n",
        "        for mf in range(len(self.memFuncs[inputVar])):\n",
        "            if self.memFuncs[inputVar][mf][0] == 'gaussmf':\n",
        "                y = gaussmf(x,**self.memClass.MFList[inputVar][mf][1])\n",
        "            elif self.memFuncs[inputVar][mf][0] == 'gbellmf':\n",
        "                y = gbellmf(x,**self.memClass.MFList[inputVar][mf][1])\n",
        "            elif self.memFuncs[inputVar][mf][0] == 'sigmf':\n",
        "                y = sigmf(x,**self.memClass.MFList[inputVar][mf][1])\n",
        "\n",
        "            plt.plot(x,y,'r')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def plotResults(self):\n",
        "        if self.trainingType == 'Not trained yet':\n",
        "            print(self.trainingType)\n",
        "        else:\n",
        "            fitted_values_int = [int(value) for value in self.fittedValues]\n",
        "            if len(fitted_values_int)<86 and fitted_values_int < 1:\n",
        "              fitted_values_int=1\n",
        "\n",
        "            print(\"o/p:\",fitted_values_int)\n",
        "            '''plt.scatter(range(0,len(fitted_values_int),5), fitted_values_int, color='r', label='trained')\n",
        "            plt.plot(range(len(self.Y)), self.Y, 'b', label='original')\n",
        "            plt.legend(loc='upper left')\n",
        "            #plt.show()'''\n",
        "\n",
        "\n",
        "\n",
        "def forwardHalfPass(ANFISObj, Xs):\n",
        "    layerFour = np.empty(0,)\n",
        "    wSum = []\n",
        "\n",
        "    for pattern in range(len(Xs[:,0])):\n",
        "        #layer one\n",
        "        layerOne = ANFISObj.memClass.evaluateMF(Xs[pattern,:])\n",
        "\n",
        "        #layer two\n",
        "        miAlloc = [[layerOne[x][ANFISObj.rules[row][x]] for x in range(len(ANFISObj.rules[0]))] for row in range(len(ANFISObj.rules))]\n",
        "        layerTwo = np.array([np.prod(x) for x in miAlloc]).T\n",
        "        if pattern == 0:\n",
        "            w = layerTwo\n",
        "        else:\n",
        "            w = np.vstack((w,layerTwo))\n",
        "\n",
        "        #layer three\n",
        "        wSum.append(np.sum(layerTwo))\n",
        "        if pattern == 0:\n",
        "            wNormalized = layerTwo/wSum[pattern]\n",
        "        else:\n",
        "            wNormalized = np.vstack((wNormalized,layerTwo/wSum[pattern]))\n",
        "\n",
        "        #prep for layer four (bit of a hack)\n",
        "        layerThree = layerTwo/wSum[pattern]\n",
        "        rowHolder = np.concatenate([x*np.append(Xs[pattern,:],1) for x in layerThree])\n",
        "        layerFour = np.append(layerFour,rowHolder)\n",
        "\n",
        "    w = w.T\n",
        "    wNormalized = wNormalized.T\n",
        "\n",
        "    layerFour = np.array(np.array_split(layerFour,pattern + 1))\n",
        "\n",
        "    return layerFour, wSum, w\n",
        "\n",
        "\n",
        "def backprop(ANFISObj, columnX, columns, theWSum, theW, theLayerFive):\n",
        "\n",
        "    paramGrp = [0]* len(ANFISObj.memFuncs[columnX])\n",
        "    for MF in range(len(ANFISObj.memFuncs[columnX])):\n",
        "\n",
        "        parameters = np.empty(len(ANFISObj.memFuncs[columnX][MF][1]))\n",
        "        timesThru = 0\n",
        "        for alpha in sorted(ANFISObj.memFuncs[columnX][MF][1].keys()):\n",
        "\n",
        "            bucket3 = np.empty(len(ANFISObj.X))\n",
        "            for rowX in range(len(ANFISObj.X)):\n",
        "                varToTest = ANFISObj.X[rowX,columnX]\n",
        "                tmpRow = np.empty(len(ANFISObj.memFuncs))\n",
        "                tmpRow.fill(varToTest)\n",
        "\n",
        "                bucket2 = np.empty(ANFISObj.Y.ndim)\n",
        "                for colY in range(ANFISObj.Y.ndim):\n",
        "\n",
        "                    rulesWithAlpha = np.array(np.where(ANFISObj.rules[:,columnX]==MF))[0]\n",
        "                    adjCols = np.delete(columns,columnX)\n",
        "\n",
        "                    senSit = partial_dMF(ANFISObj.X[rowX,columnX],ANFISObj.memFuncs[columnX][MF],alpha)\n",
        "                    # produces d_ruleOutput/d_parameterWithinMF\n",
        "                    dW_dAplha = senSit * np.array([np.prod([ANFISObj.memClass.evaluateMF(tmpRow)[c][ANFISObj.rules[r][c]] for c in adjCols]) for r in rulesWithAlpha])\n",
        "\n",
        "                    bucket1 = np.empty(len(ANFISObj.rules[:,0]))\n",
        "                    for consequent in range(len(ANFISObj.rules[:,0])):\n",
        "                        fConsequent = np.dot(np.append(ANFISObj.X[rowX,:],1.),ANFISObj.consequents[((ANFISObj.X.shape[1] + 1) * consequent):(((ANFISObj.X.shape[1] + 1) * consequent) + (ANFISObj.X.shape[1] + 1)),colY])\n",
        "                        acum = 0\n",
        "                        if consequent in rulesWithAlpha:\n",
        "                            acum = dW_dAplha[np.where(rulesWithAlpha==consequent)] * theWSum[rowX]\n",
        "\n",
        "                        acum = acum - theW[consequent,rowX] * np.sum(dW_dAplha)\n",
        "                        acum = acum / theWSum[rowX]**2\n",
        "                        bucket1[consequent] = fConsequent * acum\n",
        "\n",
        "                    sum1 = np.sum(bucket1)\n",
        "\n",
        "                    if ANFISObj.Y.ndim == 1:\n",
        "                        bucket2[colY] = sum1 * (ANFISObj.Y[rowX]-theLayerFive[rowX,colY])*(-2)\n",
        "                    else:\n",
        "                        bucket2[colY] = sum1 * (ANFISObj.Y[rowX,colY]-theLayerFive[rowX,colY])*(-2)\n",
        "\n",
        "                sum2 = np.sum(bucket2)\n",
        "                bucket3[rowX] = sum2\n",
        "\n",
        "            sum3 = np.sum(bucket3)\n",
        "            parameters[timesThru] = sum3\n",
        "            timesThru = timesThru + 1\n",
        "\n",
        "        paramGrp[MF] = parameters\n",
        "\n",
        "    return paramGrp\n",
        "\n",
        "\n",
        "def predict(ANFISObj, varsToTest):\n",
        "\n",
        "    [layerFour, wSum, w] = forwardHalfPass(ANFISObj, varsToTest)\n",
        "\n",
        "    #layer five\n",
        "    layerFive = np.dot(layerFour,ANFISObj.consequents)\n",
        "\n",
        "    return layerFive\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"I am main!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ioUCQ4I23DOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f3cbe7-a283-4897-fd99-bcd0c7da87c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current error: 3.650795183203649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-efafe332c232>:252: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  bucket1[consequent] = fConsequent * acum\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current error: 3.674042528573105\n",
            "current error: 3.67946208090824\n",
            "current error: 3.763506644324156\n",
            "current error: 3.7895835066884054\n",
            "current error: 3.7995265302936345\n",
            "current error: 3.8071482724070917\n",
            "current error: 3.8122995965383346\n",
            "current error: 3.816183690973781\n",
            "current error: 3.8193463237303003\n",
            "current error: 3.8219678510155903\n",
            "current error: 3.824147448543791\n",
            "current error: 3.8259601358195803\n",
            "current error: 3.8274674766136267\n",
            "current error: 3.8287205995102833\n",
            "current error: 3.8297620233700576\n",
            "current error: 3.8306272198239624\n",
            "current error: 3.8313459798740612\n",
            "current error: 3.8319435716007235\n",
            "o/p: [1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 3, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 3, 2, 2, 2, 1, 2, 1, 3, 3, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 1, 3, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 4, 2, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 4, 2, 3, 3, 4, 4, 3, 2, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-efafe332c232>:170: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  fitted_values_int = [int(value) for value in self.fittedValues]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy\n",
        "\n",
        "ts = numpy.loadtxt(\"/content/drive/MyDrive/Dataset_U/glcm_features.txt\", usecols=[0,1,2,3,4,5,6])\n",
        "X = ts[:,0:6]\n",
        "Y = ts[:,6]\n",
        "\n",
        "mf = [[['gaussmf', {'mean': 0.08362658101718731, 'sigma': 0.036563156293450534}],['gaussmf', {'mean':0.33357296548580995, 'sigma':0.09687164138000348}]],\n",
        "            [['gaussmf', {'mean': 0.5973780501617197, 'sigma': 0.08123594716913514}],['gaussmf', {'mean': 0.8023904228060013, 'sigma': 0.04697272639566385}]],\n",
        "             [['gaussmf', {'mean': 0.7731435514794944, 'sigma': 0.07517313917389094}],['gaussmf', {'mean': 0.886220784218452, 'sigma': 0.0255663688016036}]],\n",
        "              [['gaussmf', {'mean': 0.4311673220273308, 'sigma': 0.11959700743527502}],['gaussmf', {'mean': 1.6441466549776973, 'sigma':0.7237128672289167}]],\n",
        "            [['gaussmf', {'mean':0.24245534934272, 'sigma': 0.049435317559267805}],['gaussmf', {'mean': 0.4832042112929316, 'sigma': 0.08886447147306493}]],\n",
        "             [['gaussmf', {'mean':  0.008330269450763485, 'sigma': 0.006977763794693757}],['gaussmf', {'mean':  0.12065503820665334, 'sigma': 0.0723968717322495}]]]\n",
        "\n",
        "mfc = MemFuncs(mf)\n",
        "anf = ANFIS(X, Y, mfc)\n",
        "anf.trainHybridJangOffLine(epochs=20)\n",
        "\n",
        "\n",
        "if round(anf.consequents[15][0],6) ==3.93336 and  round(anf.consequents[6][0],6) <0 :\n",
        "\tprint('test is good')\n",
        "\n",
        "'''print(\"Plotting errors\")\n",
        "anf.plotErrors()\n",
        "print(\"Plotting results\")'''\n",
        "anf.plotResults()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}